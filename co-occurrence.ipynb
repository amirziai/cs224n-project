{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://smartiproxy.mgmt.netflix.net/pypi\r\n",
      "Requirement already satisfied: python-Levenshtein in /anaconda3/lib/python3.7/site-packages (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from python-Levenshtein) (40.6.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments import ExperimentRunner\n",
    "import params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ExperimentRunner(params.experiment1_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0a7c63314af944219a589e5f9d998d31'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('experiment_parameters',\n",
       "  {'augment': {None},\n",
       "   'pre_train': {False},\n",
       "   'lr': {0.1},\n",
       "   'decoder_type': {'rnn'},\n",
       "   'domain_name': {'geoquery'},\n",
       "   'aug_frac': {1.0},\n",
       "   'embed_size': {128},\n",
       "   'hidden_size': {128},\n",
       "   'seed': {0},\n",
       "   'dropout': {0.3},\n",
       "   'cuda': {False},\n",
       "   'batch_size_train': {256},\n",
       "   'batch_size_dev': {128},\n",
       "   'valid_niter': {100},\n",
       "   'max_epoch': {5},\n",
       "   'beam_size': {5},\n",
       "   'max_sentence_length': {1000},\n",
       "   'encoder_type': {'brnn'},\n",
       "   'file_path_train': {'data/geo880_train600.tsv'},\n",
       "   'file_path_dev': {'data/geo880_test280.tsv'},\n",
       "   'file_path_model': {'model.bin'}}),\n",
       " ('n_jobs', 1),\n",
       " ('uuid', '0a7c63314af944219a589e5f9d998d31'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(sorted(exp.__dict__.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = exp._get_param_grid(exp.experiment_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = run.Runner(**param_grid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5e7621a769cc753f025f68a33c1fc3bad7fd9f598818f518eabdd454'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashlib.sha224(bytearray('_'.join([f'{k}_{v}' for k, v in frozenset(runner.__dict__.items())]), 'utf8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a': 1, 'c': 2, 'b': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_dict(d: Dict) -> str:\n",
    "    dict_str_rep = '_'.join([f'{key}_{d[key]}' for key in sorted(d.keys())])\n",
    "    return hashlib.sha224(bytearray(dict_str_rep, 'utf8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentResults(NamedTuple):\n",
    "    sequence_correct: int\n",
    "    sequence_total: int\n",
    "    sequence_accuracy: float\n",
    "    token_correct: int\n",
    "    token_total: int\n",
    "    token_accuracy: float\n",
    "    denotation_correct: Optional[int]\n",
    "    denotation_total: Optional[int]\n",
    "    denotation_accuracy: Optional[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ExperimentResults(1, 2, 3, 4, 5, 6, 7, 8, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a._asdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('sequence_correct', 1),\n",
       "             ('sequence_total', 2),\n",
       "             ('sequence_accuracy', 3),\n",
       "             ('token_correct', 4),\n",
       "             ('token_total', 5),\n",
       "             ('token_accuracy', 6),\n",
       "             ('denotation_correct', 7),\n",
       "             ('denotation_total', 8),\n",
       "             ('denotation_accuracy', 9)])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__new__() missing 8 required positional arguments: 'sequence_total', 'sequence_accuracy', 'token_correct', 'token_total', 'token_accuracy', 'denotation_correct', 'denotation_total', and 'denotation_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-03796c6ca87f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mExperimentResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __new__() missing 8 required positional arguments: 'sequence_total', 'sequence_accuracy', 'token_correct', 'token_total', 'token_accuracy', 'denotation_correct', 'denotation_total', and 'denotation_accuracy'"
     ]
    }
   ],
   "source": [
    "ExperimentResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "        self.uuid = self._get_uuid()\n",
    "        self.c = a + 1\n",
    "        \n",
    "    def _get_uuid(self):\n",
    "        return hash_dict(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = A(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'62357b2f9297ef4295f9c41d8f259a9f0c100d1ff67b518ef3525a52'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.c = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'62357b2f9297ef4295f9c41d8f259a9f0c100d1ff67b518ef3525a52'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 5, 'b': {...}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.b['b']['b']['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/geo880_train600.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is the highest point in florida ?',\n",
       " '_answer ( A , _highest ( A , ( _place ( A ) , _loc ( A , B ) , _const ( B , _stateid ( florida ) ) ) ) )']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tgt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('largest', '_largest'): 9,\n",
       "         ('smallest', '_smallest'): 4,\n",
       "         ('highest', '_highest'): 2,\n",
       "         ('longest', '_longest'): 2,\n",
       "         ('most', '_most'): 5,\n",
       "         ('capital', '_capital'): 1,\n",
       "         ('states', '_state'): 3,\n",
       "         ('state', '_state'): 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tgt = [line.strip().split('\\t') for line in open(data)]\n",
    "\n",
    "Counter([\n",
    "    (x, y)\n",
    "    for src, tgt in src_tgt\n",
    "    for i, (x, y) in enumerate(zip(src.split(), tgt.split()))\n",
    "    if distance(x, y) <= 2 and len(x) >= 3 and len(y) >= 3\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends = set()\n",
    "friends.union({1, 2, 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends = []\n",
    "\n",
    "window_size = 5\n",
    "distance_threshold = 3\n",
    "token_size_threshold = 3\n",
    "\n",
    "for src, tgt in src_tgt:\n",
    "    src_tokens = src.split()\n",
    "    tgt_tokens = tgt.split()\n",
    "    \n",
    "    for i, src_token in enumerate(src_tokens):\n",
    "        if len(src_token) >= token_size_threshold:\n",
    "            friends += [\n",
    "                (src_token, tgt_tokens[i + j])\n",
    "                for j in range(-window_size, window_size + 1)\n",
    "                if (0 <= i + j < len(tgt_tokens) and len(tgt_tokens[i + j]) >= token_size_threshold and distance(tgt_tokens[i + j], src_token) <= distance_threshold)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('highest', '_highest'): 26,\n",
       "         ('state', '_state'): 62,\n",
       "         ('mountain', '_mountain'): 4,\n",
       "         ('capital', '_capital'): 25,\n",
       "         ('populations', '_population'): 7,\n",
       "         ('name', '_lake'): 1,\n",
       "         ('lakes', '_lake'): 4,\n",
       "         ('states', '_state'): 96,\n",
       "         ('population', '_population'): 37,\n",
       "         ('rivers', '_river'): 31,\n",
       "         ('lowest', '_lowest'): 14,\n",
       "         ('capitals', '_capital'): 4,\n",
       "         ('size', '_size'): 5,\n",
       "         ('shortest', '_shortest'): 9,\n",
       "         ('river', '_river'): 32,\n",
       "         ('major', '_major'): 27,\n",
       "         ('most', '_most'): 13,\n",
       "         ('long', '_len'): 7,\n",
       "         ('the', '_len'): 18,\n",
       "         ('area', '_area'): 15,\n",
       "         ('longest', '_longest'): 21,\n",
       "         ('smallest', '_smallest'): 21,\n",
       "         ('largest', '_largest'): 42,\n",
       "         ('city', '_city'): 26,\n",
       "         ('over', '_river'): 1,\n",
       "         ('are', '_lake'): 1,\n",
       "         ('density', '_density'): 8,\n",
       "         ('mountains', '_mountain'): 1,\n",
       "         ('elevation', '_elevation'): 2,\n",
       "         ('found', '_count'): 1,\n",
       "         ('miles', '_len'): 1,\n",
       "         ('with', '_city'): 2})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = [\n",
    "    (x, y)\n",
    "    for src, tgt in src_tgt\n",
    "    for i, (x, y) in enumerate(zip(src.split(), tgt.split()))\n",
    "    # if distance(x, y) <= 2 and len(x) >= 3 and len(y) >= 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = defaultdict(lambda: defaultdict(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src, tgt in tuples:\n",
    "    if len(src) >= 3 and len(tgt) >= 3:\n",
    "        cnt[tgt][src] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "are\n",
      "border\n",
      "pennsylvania\n",
      "usa\n",
      "virginia\n",
      "texas\n",
      "citizens\n",
      "borders\n",
      "california\n",
      "population\n"
     ]
    }
   ],
   "source": [
    "for src in cnt['_city']:\n",
    "    print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_answer': {'what': 384, 'which': 53, 'how': 111, 'where': 17},\n",
       " '_largest': {'state': 13, 'city': 19}}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lexicon(src_tgt, is_reverse=False):\n",
    "    cnt = defaultdict(lambda: defaultdict(int))\n",
    "    cnt_tgt = defaultdict(int)\n",
    "\n",
    "    window_size = 3\n",
    "    # distance_threshold = 3\n",
    "    token_size_threshold = 3\n",
    "    support = 0.3\n",
    "    support_abs = 2\n",
    "    stop_words = set(stopwords.words('english')).union({'_answer'} if is_reverse else set())\n",
    "\n",
    "\n",
    "    for src, tgt in src_tgt:\n",
    "        src_tokens = src.split()\n",
    "        tgt_tokens = tgt.split()\n",
    "\n",
    "        for tgt_token in tgt_tokens:\n",
    "            cnt_tgt[tgt_token] += 1\n",
    "\n",
    "        for i, src_token in enumerate(src_tokens):\n",
    "            if len(src_token) >= token_size_threshold and src_token not in stop_words:\n",
    "                for j in range(-window_size, window_size + 1):\n",
    "                    if (0 <= i + j < len(tgt_tokens) and len(tgt_tokens[i + j]) >= token_size_threshold and tgt_tokens[i + j] not in stop_words):\n",
    "                        cnt[tgt_tokens[i + j]][src_token] += 1\n",
    "\n",
    "    x = {\n",
    "        tgt: {src: cnt[tgt][src] for src in cnt[tgt] if cnt[tgt][src] >= support * cnt_tgt[tgt] and cnt[tgt][src] >= support_abs}\n",
    "        for tgt in cnt\n",
    "        if len({src for src in cnt[tgt] if cnt[tgt][src] >= support * cnt_tgt[tgt] and cnt[tgt][src] >= support_abs}) >= 2\n",
    "    }\n",
    "    \n",
    "    out = defaultdict(set)\n",
    "    for val_dict in x.values():\n",
    "        val_vals = val_dict.keys()\n",
    "        for val in val_vals:\n",
    "            for val_map_to in val_vals:\n",
    "                if val_map_to != val:\n",
    "                    out[val].add(val_map_to)\n",
    "                    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample([(1, 2), (3, 4), (5, 6)], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6232124459954171"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-891e3400ee9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object does not support indexing"
     ]
    }
   ],
   "source": [
    "random.choice({1, 2, 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_src = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'highest': {'point'},\n",
       "             'point': {'highest', 'lowest'},\n",
       "             'lakes': {'states'},\n",
       "             'states': {'borders',\n",
       "              'cities',\n",
       "              'lakes',\n",
       "              'least',\n",
       "              'many',\n",
       "              'population',\n",
       "              'rivers',\n",
       "              'state'},\n",
       "             'lowest': {'point'},\n",
       "             'many': {'cities', 'rivers', 'states'},\n",
       "             'cities': {'major', 'many', 'rivers', 'states'},\n",
       "             'rivers': {'cities', 'many', 'states'},\n",
       "             'shortest': {'river'},\n",
       "             'river': {'length', 'longest', 'shortest'},\n",
       "             'major': {'cities'},\n",
       "             'state': {'borders',\n",
       "              'largest',\n",
       "              'least',\n",
       "              'population',\n",
       "              'smallest',\n",
       "              'states'},\n",
       "             'length': {'river'},\n",
       "             'longest': {'river'},\n",
       "             'smallest': {'population', 'state'},\n",
       "             'population': {'density', 'smallest', 'state', 'states'},\n",
       "             'largest': {'state'},\n",
       "             'density': {'population'},\n",
       "             'borders': {'least', 'state', 'states'},\n",
       "             'least': {'borders', 'state', 'states'}})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lexicon(src_tgt, is_reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'_state': {'_count',\n",
       "              '_density',\n",
       "              '_largest',\n",
       "              '_population',\n",
       "              '_river'},\n",
       "             '_river': {'_city', '_longest', '_population', '_state'},\n",
       "             '_lake': {'_major'},\n",
       "             '_major': {'_count', '_lake'},\n",
       "             '_population': {'_river', '_state'},\n",
       "             '_count': {'_major', '_state'},\n",
       "             '_largest': {'_density', '_state'},\n",
       "             '_density': {'_area', '_largest', '_state'},\n",
       "             '_place': {'_loc'},\n",
       "             '_loc': {'_place'},\n",
       "             '_area': {'_density'},\n",
       "             '_longest': {'_river'},\n",
       "             '_city': {'_river'},\n",
       "             '_smallest': {'_fewest'},\n",
       "             '_fewest': {'_smallest'}})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lexicon(tgt_src, is_reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Set\n",
    "import random\n",
    "\n",
    "stopwords = {'a',\n",
    "             'about',\n",
    "             'above',\n",
    "             'after',\n",
    "             'again',\n",
    "             'against',\n",
    "             'ain',\n",
    "             'all',\n",
    "             'am',\n",
    "             'an',\n",
    "             'and',\n",
    "             'any',\n",
    "             'are',\n",
    "             'aren',\n",
    "             \"aren't\",\n",
    "             'as',\n",
    "             'at',\n",
    "             'be',\n",
    "             'because',\n",
    "             'been',\n",
    "             'before',\n",
    "             'being',\n",
    "             'below',\n",
    "             'between',\n",
    "             'both',\n",
    "             'but',\n",
    "             'by',\n",
    "             'can',\n",
    "             'couldn',\n",
    "             \"couldn't\",\n",
    "             'd',\n",
    "             'did',\n",
    "             'didn',\n",
    "             \"didn't\",\n",
    "             'do',\n",
    "             'does',\n",
    "             'doesn',\n",
    "             \"doesn't\",\n",
    "             'doing',\n",
    "             'don',\n",
    "             \"don't\",\n",
    "             'down',\n",
    "             'during',\n",
    "             'each',\n",
    "             'few',\n",
    "             'for',\n",
    "             'from',\n",
    "             'further',\n",
    "             'had',\n",
    "             'hadn',\n",
    "             \"hadn't\",\n",
    "             'has',\n",
    "             'hasn',\n",
    "             \"hasn't\",\n",
    "             'have',\n",
    "             'haven',\n",
    "             \"haven't\",\n",
    "             'having',\n",
    "             'he',\n",
    "             'her',\n",
    "             'here',\n",
    "             'hers',\n",
    "             'herself',\n",
    "             'him',\n",
    "             'himself',\n",
    "             'his',\n",
    "             'how',\n",
    "             'i',\n",
    "             'if',\n",
    "             'in',\n",
    "             'into',\n",
    "             'is',\n",
    "             'isn',\n",
    "             \"isn't\",\n",
    "             'it',\n",
    "             \"it's\",\n",
    "             'its',\n",
    "             'itself',\n",
    "             'just',\n",
    "             'll',\n",
    "             'm',\n",
    "             'ma',\n",
    "             'me',\n",
    "             'mightn',\n",
    "             \"mightn't\",\n",
    "             'more',\n",
    "             'most',\n",
    "             'mustn',\n",
    "             \"mustn't\",\n",
    "             'my',\n",
    "             'myself',\n",
    "             'needn',\n",
    "             \"needn't\",\n",
    "             'no',\n",
    "             'nor',\n",
    "             'not',\n",
    "             'now',\n",
    "             'o',\n",
    "             'of',\n",
    "             'off',\n",
    "             'on',\n",
    "             'once',\n",
    "             'only',\n",
    "             'or',\n",
    "             'other',\n",
    "             'our',\n",
    "             'ours',\n",
    "             'ourselves',\n",
    "             'out',\n",
    "             'over',\n",
    "             'own',\n",
    "             're',\n",
    "             's',\n",
    "             'same',\n",
    "             'shan',\n",
    "             \"shan't\",\n",
    "             'she',\n",
    "             \"she's\",\n",
    "             'should',\n",
    "             \"should've\",\n",
    "             'shouldn',\n",
    "             \"shouldn't\",\n",
    "             'so',\n",
    "             'some',\n",
    "             'such',\n",
    "             't',\n",
    "             'than',\n",
    "             'that',\n",
    "             \"that'll\",\n",
    "             'the',\n",
    "             'their',\n",
    "             'theirs',\n",
    "             'them',\n",
    "             'themselves',\n",
    "             'then',\n",
    "             'there',\n",
    "             'these',\n",
    "             'they',\n",
    "             'this',\n",
    "             'those',\n",
    "             'through',\n",
    "             'to',\n",
    "             'too',\n",
    "             'under',\n",
    "             'until',\n",
    "             'up',\n",
    "             've',\n",
    "             'very',\n",
    "             'was',\n",
    "             'wasn',\n",
    "             \"wasn't\",\n",
    "             'we',\n",
    "             'were',\n",
    "             'weren',\n",
    "             \"weren't\",\n",
    "             'what',\n",
    "             'when',\n",
    "             'where',\n",
    "             'which',\n",
    "             'while',\n",
    "             'who',\n",
    "             'whom',\n",
    "             'why',\n",
    "             'will',\n",
    "             'with',\n",
    "             'won',\n",
    "             \"won't\",\n",
    "             'wouldn',\n",
    "             \"wouldn't\",\n",
    "             'y',\n",
    "             'you',\n",
    "             \"you'd\",\n",
    "             \"you'll\",\n",
    "             \"you're\",\n",
    "             \"you've\",\n",
    "             'your',\n",
    "             'yours',\n",
    "             'yourself',\n",
    "             'yourselves'}\n",
    "\n",
    "\n",
    "CoOccurrenceData = List[Tuple[str, str]]\n",
    "\n",
    "class CoOccurrence:\n",
    "    def __init__(self, src_tgt: CoOccurrenceData):\n",
    "        self.window_size = 3\n",
    "        self.token_size_threshold = 3\n",
    "        self.support = 0.3\n",
    "        self.support_abs = 2\n",
    "        self.aug_prob = 0.5\n",
    "\n",
    "        self.src_tgt = src_tgt\n",
    "        self.lexicon_src_tgt = self._generate_lexicon(self.src_tgt, is_reverse=False)\n",
    "        self.lexicon_tgt_src = self._generate_lexicon([(y, x) for x, y in self.src_tgt], is_reverse=True)\n",
    "\n",
    "    def _generate_lexicon(self, src_tgt: List[Tuple[str, str]], is_reverse: bool = False) -> Dict[str, Set[str]]:\n",
    "        cnt = defaultdict(lambda: defaultdict(int))\n",
    "        cnt_tgt = defaultdict(int)\n",
    "\n",
    "        # _answer appears everywhere for target -> source\n",
    "        stop_words = set(stopwords).union({'_answer'} if is_reverse else set())\n",
    "\n",
    "        for src, tgt in src_tgt:\n",
    "            src_tokens = src.split()\n",
    "            tgt_tokens = tgt.split()\n",
    "\n",
    "            for tgt_token in tgt_tokens:\n",
    "                cnt_tgt[tgt_token] += 1\n",
    "\n",
    "            for i, src_token in enumerate(src_tokens):\n",
    "                if len(src_token) >= self.token_size_threshold and src_token not in stop_words:\n",
    "                    for j in range(-self.window_size, self.window_size + 1):\n",
    "                        if (0 <= i + j < len(tgt_tokens) and len(tgt_tokens[i + j]) >= self.token_size_threshold and\n",
    "                                tgt_tokens[i + j] not in stop_words):\n",
    "                            cnt[tgt_tokens[i + j]][src_token] += 1\n",
    "\n",
    "        # set of src words that positionally (within a window) co-occur with some min support with the target\n",
    "        x = {\n",
    "            tgt: {src: cnt[tgt][src] for src in cnt[tgt] if\n",
    "                  cnt[tgt][src] >= self.support * cnt_tgt[tgt] and cnt[tgt][src] >= self.support_abs}\n",
    "            for tgt in cnt\n",
    "            if\n",
    "            len({src for src in cnt[tgt] if\n",
    "                 cnt[tgt][src] >= self.support * cnt_tgt[tgt] and cnt[tgt][src] >= self.support_abs}) >= 2\n",
    "        }\n",
    "\n",
    "        # lookup for co-occurring tokens\n",
    "        out = defaultdict(set)\n",
    "        for val_dict in x.values():\n",
    "            val_vals = val_dict.keys()\n",
    "            for val in val_vals:\n",
    "                for val_map_to in val_vals:\n",
    "                    if val_map_to != val:\n",
    "                        out[val].add(val_map_to)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _aug(self, token: str, src: bool) -> str:\n",
    "        lexicon = self.lexicon_src_tgt if src else self.lexicon_tgt_src\n",
    "        if token in lexicon:\n",
    "            do_aug = random.random() <= self.aug_prob\n",
    "            return random.choice(list(lexicon[token])) if do_aug else token\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "    def _sample_item(self, x_str: str, y_str: str) -> Tuple[str, str]:\n",
    "        x_lst, y_lst = x_str.split(), y_str.split()\n",
    "        xs_aug = ' '.join([self._aug(x, src=True) for x in x_lst])\n",
    "        ys_aug = ' '.join([self._aug(y, src=False) for y in y_lst])\n",
    "        return xs_aug, ys_aug\n",
    "\n",
    "    def sample(self, n) -> CoOccurrenceData:\n",
    "        src_tgt_n = random.sample(self.src_tgt, n)\n",
    "        return [self._sample_item(x_str, y_str) for x_str, y_str in src_tgt_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = CoOccurrence(src_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'_state': {'_count',\n",
       "              '_density',\n",
       "              '_largest',\n",
       "              '_population',\n",
       "              '_river'},\n",
       "             '_river': {'_city', '_longest', '_population', '_state'},\n",
       "             '_lake': {'_major'},\n",
       "             '_major': {'_count', '_lake'},\n",
       "             '_population': {'_river', '_state'},\n",
       "             '_count': {'_major', '_state'},\n",
       "             '_largest': {'_density', '_state'},\n",
       "             '_density': {'_area', '_largest', '_state'},\n",
       "             '_place': {'_loc'},\n",
       "             '_loc': {'_place'},\n",
       "             '_area': {'_density'},\n",
       "             '_longest': {'_river'},\n",
       "             '_city': {'_river'},\n",
       "             '_smallest': {'_fewest'},\n",
       "             '_fewest': {'_smallest'}})"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.lexicon_tgt_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what is the biggest city in usa ?',\n",
       "  '_answer ( A , _largest ( A , ( _city ( A ) , _loc ( A , B ) , _const ( B , _countryid ( usa ) ) ) ) )'),\n",
       " ('what cities in texas have the highest number of citizens ?',\n",
       "  '_answer ( A , _density ( B , ( _city ( A ) , _loc ( A , C ) , _const ( C , _stateid ( texas ) ) , _state ( A , B ) ) ) )'),\n",
       " ('what states border hawaii ?',\n",
       "  '_answer ( A , ( _state ( A ) , _next_to ( A , B ) , _const ( B , _stateid ( hawaii ) ) ) )'),\n",
       " ('where is mount whitney ?',\n",
       "  \"_answer ( A , ( _place ( B , A ) , _const ( B , _placeid ( ' mount whitney ' ) ) ) )\"),\n",
       " ('where is the lowest spot in iowa ?',\n",
       "  '_answer ( A , _lowest ( A , ( _loc ( A ) , _place ( A , B ) , _const ( B , _stateid ( iowa ) ) ) ) )')]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
